### 线性回归

#### 1. 线性回归的基本要素

##### 1.1 线性回归模型简介

我们的目标是希望探索目标与特征之间的具体关系。线性回归模型假设输出与各项输入之间是呈现线性关系。课件中的例子假设房价只与房屋面积和房龄成线性关系，公式如下：
$$
\mathrm{price} = w_{\mathrm{area}} \cdot \mathrm{area} + w_{\mathrm{age}} \cdot \mathrm{age} + b
$$

##### 1.2 数据集

数据集分为训练集和验证集，数据集的单条数据被称为样本(sample)，目标称作标签(label)，用来预测标签的两个因素叫做特征(feature)

##### 1.3 损失函数

损失函数用来评估预测值与真实值(标签值)之间的误差。数值越小代表误差越小，一个常用的选择是平方函数，即使用真实值与预测值差值的平方来刻画。批量样本的损失即为单个样本损失之和。

$$
l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2,
$$

$$
L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.
$$

##### 1.4 优化函数 -- 随机梯度下降( SGD )

什么是优化函数？

- 优化函数是用来迭代模型参数以降低损失

什么是解析解和数值解？

- 当模型与损失函数较为简单时，以上的误差最小化问题可以直接用公式表达出来，这类解称作解析解(analytical solution)。例如本届的线性回归和平方误差
- 不过，大多数基于深度学习的算法并没有解析解，只有通过优化算法来迭代模型参数来尽可能降低损失，这类解叫数值解(numerical solution)

什么是小批量随机梯度下降(mini-batch stochastic gradient descent)？

- 首先，选取一组模型参数的初始值，例如随机选取
- 在每次迭代中，随机均匀采样一个小批量(mini-batch) $\mathcal{B}$ ，然后求小批量中数据样本的平均损失有关模型参数的导数(梯度)
- 用梯度与预先设定的正数 $\eta$ (又称作学习率)的乘积作为模型参数在本次迭代的减小量(沿着负梯度方向来移动参数)

#### 2. 张量计算(个人认为应该是张量)

模型训练时往往是对张量进行计算，张量计算的方法主要有如下两种

- 将这两个张量按元素逐一做标量加法。
- 将这两个张量直接做张量加法。

Code文件夹中对张量计算做了计算效率对比，使用张量加法的效率会高很多，尽量采取这种方法。

#### 3. 线性回归模型从零开始实现

- 代码在Code文件夹中

#### 4. 线性回归模型使用pytorch的简洁实现

- 代码在Code文件夹中