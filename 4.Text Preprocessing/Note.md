### 文本预处理

#### 1. 简介

文本是序列数据，一篇文章可以看作是字符或者单词的序列。在进行自然语言处理的任务前，通常需要进行文本数据的预处理，预处理通常包含四个步骤

- 读入文本
- 分词
- 建立字典，将每个词映射到一个唯一的索引(index)
- 将文本从词的序列转换为索引的序列，方便作为模型的输入

#### 2. 读入文本及分词

读入文本通常是读入txt文件，预处理模型通常使用python的正则表达式库re，并且将其转化为较为统一的格式(例如全部转为小写)

#### 3. 建立字典及转为索引

建立字典即为将词转换为索引，将每个词映射到一个唯一的索引标号。在建立好字典后，每个词可以通过查字典转为索引，转为索引后可作为模型输入

#### 4. 分词的现有工具

在实例中介绍了两个分词工具：spaCy 和 NLTK，此外我了解过三款中文分词，也列举如下

- [jieba--用来做中文分词](https://github.com/fxsjy/jieba)
- [THULAC--清华大学推出的中文词法分析工具包](http://thulac.thunlp.org/)

- [pkuseg--北大推出的工具包号称实验结果超过以上两者](https://github.com/lancopku/PKUSeg-python)

