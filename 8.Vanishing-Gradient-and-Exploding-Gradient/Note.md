### 梯度消失和梯度爆炸

#### 1. 梯度消失和梯度爆炸

- 什么是梯度消失?

当梯度很小并且网络较深时，多项较小梯度连乘会导致梯度趋于零

- 什么是梯度爆炸?

当梯度较大且网络较深时，多项较大梯度连乘会导致梯度爆炸

#### 2. 随机初始化模型参数

- 为什么要随机初始化模型参数

如果每个隐藏单元初始化为相同的初始值，并且该神经元前后单元与周围神经元相同，那么在反向传播中每个隐藏单元的参数梯度值相等，在优化算法(梯度下降算法)迭代之后其参数依然相等，在这种情况下，无论隐藏单元有多少，本质上还是只有一个隐藏单元在发挥作用。因此，我们有必要对模型参数做随机初始化

##### 2.1 Pytorch的默认随机初始化

在线性回归的简洁实现中，我们使用`torch.nn.init.normal_()`使模型`net`的权重参数采用正态分布的随机初始化方式。不过，PyTorch中`nn.Module`的模块参数都采取了较为合理的初始化策略

##### 2.2 Xavier随机初始化

还有一种比较常用的随机初始化方法叫作Xavier随机初始化。 假设某全连接层的输入个数为𝑎a，输出个数为𝑏b，Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布
$$
U\left(-\sqrt{\frac{6}{a+b}}, \sqrt{\frac{6}{a+b}}\right).
$$
它的设计主要考虑到，模型参数初始化后，每层输出的方差不该受该层输入个数影响，且每层梯度的方差也不该受该层输出个数影响。

##### 2.3 Kaiming随机初始化 -- 号称对ReLU激活函数更好

#### 3. 考虑环境因素

##### 3.1 协变量偏移

eg. 真实世界的猫和动漫猫

##### 3.2 标签偏移

eg. 训练集中只包含流感$A$，但测试集中却包含流感$A$和流感$B$

##### 3.3 概念偏移

eg. 不同地区的soft drink不一样

#### 4.Kaggle 房价预测